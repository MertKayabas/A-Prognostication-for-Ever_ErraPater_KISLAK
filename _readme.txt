Title: A Prognostication for Ever
Author: Erra Pater (pseudonym)
Place of Publication: London, England
Published 1535
University of Pennsylvania, Kislak Center for Special Collections, Rare Books and Manuscripts, BF1681 .E7 1694
Colenda Link: https://colenda.library.upenn.edu/catalog/81431-p3c824q7c
Catelog Link: https://find.library.upenn.edu/catalog/9972319813503681?hld_id=resource_link_0

For this project on “A Prognostication for Ever” and the collection of metadata, I only selected the file name, unique identifier, page number, iiif canvas id, image width and height, and file size as the metadata to record. This captures the essential information that is needed to locate the digital images of the pages and some other details. I focused on what was most reliable and relevant for archival purposes, which was easily identifiable within the Visual Data Coder. To avoid recording potentially inaccurate or misleading information, I stuck to what was accessible in the Visual Data Coder. This metadata gives a foundation to build off of when studying or referencing this given book, and anyone who views the data set can trace the origins of a page through the IIIF and look at specific information like the size of the file and the dimensions of the images. There are pretty interesting differences in the size and dimensions between the first couple of pages and the rest of the pages that I selected.

Clean and Extract Text:

I transcribed the first 25 pages of Erra Pater: A Prognostication for Ever from the cover, which I counted as page 1, to the 25th page. I initially began with Adobe Acrobat OCR and quickly realized that this would not work for this kind of manuscript. It was spitting out a transcript that was completely incorrect and made no sense, and I decided to move on to Transkribus. Since it was capable of using other models of handwriting, I tried it initially with English Print (18th century, since the book was written in 1694). After this initial try, it left me with incoherent text that was impossible to piece together. I tried English Handwriting Model 1 and Generic Old English Print, which both left me with the same results: lots of errors that would have been impossible to correct by myself without taking hours. It would have been faster to hand-transcribe it with the text that Transkribus and Adobe Acrobat left me with. I then moved onto ChatGPT and it performed way better than the other two software. I observed about 1-2 errors per page, with some pages having 0 errors. On page 22, there were three extra words added that changed the meaning, which I deleted, and on page 24, the final word on the page was omitted. I normalized all of the historical glyphs that I cannot type, such as the long S, and also omitted the alignment/centering of the words. I kept the line breaks, though. Overall, everything was preserved as much as it could be. I wrote down metadata on the media elements of the book that don't have any typing by describing the paintings and having ChatGPT describe them as well. Quick OCR, or that quick and dirty analysis, can be acceptable in certain instances for discovery, but it is not sufficient in academic settings or places like a classroom teaching. Cleaning and organizing are essential. The cost of this cleaning and labor I think, should also be borne by the institutions that benefit the most from this work, such as universities or libraries. 
